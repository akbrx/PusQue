# -*- coding: utf-8 -*-
"""PusQue prediksi poli.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rwsp45qnuCt_J-nLKV-yayJhgn330dY9

# Data Loading
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

import matplotlib.pyplot as plt
import seaborn as sns

"""## Mengunduh Dataset dari Kaggle"""

from google.colab import files
uploaded = files.upload()

import os
import shutil

# Rename file yang diupload ke kaggle.json
original_filename = next(iter(uploaded))  # Ambil nama file hasil upload
os.rename(original_filename, "kaggle.json")

# Buat folder ~/.kaggle dan pindahkan file
os.makedirs("/root/.kaggle", exist_ok=True)
shutil.move("kaggle.json", "/root/.kaggle/kaggle.json")

# Ubah permission agar tidak error
os.chmod("/root/.kaggle/kaggle.json", 600)

# Download file ZIP dari dataset
!kaggle datasets download -d bharathreddybollu/hospital-wait-time-data

# Ekstrak file ZIP
import zipfile

with zipfile.ZipFile("hospital-wait-time-data.zip", "r") as zip_ref:
    zip_ref.extractall()

"""## Memuat Data ke Pandas DataFrame"""

# Perhatikan penulisan nama file: dua spasi & huruf besar I di "TIme"
df = pd.read_csv("Hospital Wait  TIme Data.csv")

# Tampilkan beberapa baris awal
df.head()

"""# EDA (Eksplorasi Data)"""

print(df.info())
print(df.isnull().sum())

"""## Statistik Deskriptif"""

# Menampilkan statistik deskriptif untuk kolom numerik
print("Statistik Deskriptif:")
print(df.describe())

# Menampilkan statistik deskriptif untuk kolom non-numerik (terutama objek/kategorikal)
print("\nStatistik Deskriptif untuk Kolom Kategorikal:")
print(df.describe(include=['object']))

""" ## Visualisasi Matriks Korelasi"""

# Pastikan kolom 'Duration' ada dan numerik
numeric_df = df.select_dtypes(include=np.number) # Pilih hanya kolom numerik

plt.figure(figsize=(12, 8))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matriks Korelasi Fitur Numerik')
plt.show()

"""# Data Preprocessing

## Menangani Missing Values
"""

# Menampilkan jumlah missing values per kolom
df.isnull().sum()

# Mengisi missing values pada kolom 'InsuranceType' dan 'TestsOrdered' dengan 'Unknown'
df['InsuranceType'] = df['InsuranceType'].fillna('Unknown')
df['TestsOrdered'] = df['TestsOrdered'].fillna('Unknown')

# Penanganan missing pada RegistrationTime
print(f"\nMissing RegistrationTime: {df['RegistrationTime'].isna().sum()} dari {len(df)} baris")
df['RegistrationTime_missing'] = df['RegistrationTime'].isna().astype(int) # Flag missing
df['RegistrationTime'] = pd.to_datetime(df['RegistrationTime'], errors='coerce') # Konversi ke datetime
min_date = df['RegistrationTime'].min() # Cari tanggal minimum yang valid
if pd.isna(min_date):
    min_date = pd.to_datetime('2000-01-01') # Fallback jika semua NaT
df['RegistrationTime'] = df['RegistrationTime'].fillna(min_date) # Isi missing dengan tanggal minimum
print(f"Missing RegistrationTime setelah imputasi: {df['RegistrationTime'].isna().sum()}")

# Imputasi missing pada kolom durasi/waktu lainnya dengan median
cols_to_impute_median = ['RegistrationWaitTimeTime', 'RegistrationToCheckInTime'] # Tambahkan kolom lain jika perlu
for col in cols_to_impute_median:
    if col in df.columns and df[col].isnull().any():
        if pd.api.types.is_numeric_dtype(df[col]):
            median_val = df[col].median()
            df[col] = df[col].fillna(median_val)
            print(f"Missing '{col}' diisi dengan median ({median_val:.2f}).")
        else:
            print(f"WARNING: Kolom '{col}' bukan numerik, tidak bisa diisi median.")

"""## Konversi Kolom Waktu ke Datetime dan Rekayasa Fitur"""

# 2. Konversi Kolom Waktu ke Datetime dan Ekstraksi Fitur Waktu
print("\nMengkonversi Kolom Waktu ke Datetime & Ekstraksi Fitur...")
time_cols_all = ['VisitDate', 'AppointmentTime', 'ProviderStartTime', 'ProviderEndTime',
                 'ActualArrivalTime', 'CheckInTime', 'TriageCompleteTime', 'TestsCompleteTime',
                 'DischargeTime', 'ArrivalDateTime', 'FirstSeenByNurseTime', 'DoctorOrProcedureStartTime',
                 'RegistrationTime'] # Tambahkan RegistrationTime yang sudah diimputasi

for col in time_cols_all:
    if col in df.columns and not pd.api.types.is_datetime64_any_dtype(df[col]):
        # Gunakan errors='coerce' untuk mengubah format tidak valid menjadi NaT
        df[col] = pd.to_datetime(df[col], errors='coerce')
        if pd.api.types.is_datetime64_any_dtype(df[col]):
             print(f"Kolom '{col}' dikonversi ke datetime.")
        else:
             print(f"WARNING: Kolom '{col}' gagal dikonversi ke datetime.")

"""## Menghitung Kolom Target ('Duration')"""

# Hitung durasi yang relevan (dalam menit)
# Target: Duration Provider
if 'ProviderEndTime' in df.columns and 'ProviderStartTime' in df.columns and \
   pd.api.types.is_datetime64_any_dtype(df['ProviderEndTime']) and \
   pd.api.types.is_datetime64_any_dtype(df['ProviderStartTime']):
    df['Duration'] = (df['ProviderEndTime'] - df['ProviderStartTime']).dt.total_seconds() / 60.0
    print("Kolom 'Duration' (Target) dihitung.")
else:
    print("WARNING: Tidak dapat menghitung 'Duration'. Pastikan ProviderStartTime & ProviderEndTime ada dan datetime.")
    # Buat kolom 'Duration' dengan NaN jika tidak dapat dihitung, agar script selanjutnya tidak error
    df['Duration'] = np.nan

"""## Menghitung Durasi Antar Tahap Lainnya"""

# Hitung durasi antar tahap jika timestamp relevan ada
if 'ArrivalDateTime' in df.columns and 'FirstSeenByNurseTime' in df.columns and \
   pd.api.types.is_datetime64_any_dtype(df['ArrivalDateTime']) and \
   pd.api.types.is_datetime64_any_dtype(df['FirstSeenByNurseTime']):
    df['TimeFromArrivalToNurse'] = (df['FirstSeenByNurseTime'] - df['ArrivalDateTime']).dt.total_seconds() / 60.0
    df['TimeFromArrivalToNurse'] = df['TimeFromArrivalToNurse'].fillna(df['TimeFromArrivalToNurse'].median()) # Imputasi NaN
    df['TimeFromArrivalToNurse'] = df['TimeFromArrivalToNurse'].clip(lower=0) # Durasi tidak boleh negatif
    print("Fitur 'TimeFromArrivalToNurse' dihitung & diimputasi.")
else:
     df['TimeFromArrivalToNurse'] = np.nan # Buat kolom placeholder
     print("WARNING: Tidak dapat menghitung 'TimeFromArrivalToNurse'.")


if 'FirstSeenByNurseTime' in df.columns and 'DoctorOrProcedureStartTime' in df.columns and \
   pd.api.types.is_datetime64_any_dtype(df['FirstSeenByNurseTime']) and \
   pd.api.types.is_datetime64_any_dtype(df['DoctorOrProcedureStartTime']):
    df['TimeFromNurseToDoctor'] = (df['DoctorOrProcedureStartTime'] - df['FirstSeenByNurseTime']).dt.total_seconds() / 60.0
    df['TimeFromNurseToDoctor'] = df['TimeFromNurseToDoctor'].fillna(df['TimeFromNurseToDoctor'].median()) # Imputasi NaN
    df['TimeFromNurseToDoctor'] = df['TimeFromNurseToDoctor'].clip(lower=0) # Durasi tidak boleh negatif
    print("Fitur 'TimeFromNurseToDoctor' dihitung & diimputasi.")
else:
     df['TimeFromNurseToDoctor'] = np.nan # Buat kolom placeholder
     print("WARNING: Tidak dapat menghitung 'TimeFromNurseToDoctor'.")

""" ## Mengekstrak Fitur Dari Tanggal/Waktu Utama"""

# Ekstrak fitur dari AppointmentTime atau VisitDate
if 'AppointmentTime' in df.columns and pd.api.types.is_datetime64_any_dtype(df['AppointmentTime']):
    df['AppointmentHour'] = df['AppointmentTime'].dt.hour
    print("Fitur 'AppointmentHour' diekstrak.")
else:
    df['AppointmentHour'] = np.nan
    print("WARNING: Tidak dapat mengekstrak 'AppointmentHour'.")

if 'VisitDate' in df.columns and pd.api.types.is_datetime64_any_dtype(df['VisitDate']):
    df['VisitDayOfWeek'] = df['VisitDate'].dt.dayofweek # 0=Senin, ..., 6=Minggu
    df['VisitMonth'] = df['VisitDate'].dt.month
    df['VisitYear'] = df['VisitDate'].dt.year
    df['VisitDayOfMonth'] = df['VisitDate'].dt.day
    df['IsWeekend'] = (df['VisitDayOfWeek'] >= 5).astype(int)
    print("Fitur 'VisitDayOfWeek', 'VisitMonth', 'VisitYear', 'VisitDayOfMonth', 'IsWeekend' diekstrak.")
else:
    df['VisitDayOfWeek'] = np.nan
    df['VisitMonth'] = np.nan
    df['VisitYear'] = np.nan
    df['VisitDayOfMonth'] = np.nan
    df['IsWeekend'] = np.nan
    print("WARNING: Tidak dapat mengekstrak fitur dari 'VisitDate'.")

""" ## Penanganan Kolom Kategorikal"""

# 3. Penanganan Kolom Kategorikal yang Masih Object dan Kolom Binned
print("\nMenangani Kolom Kategorikal & Binned yang Tersisa...")
non_numeric_cols_after_time_features = df.select_dtypes(include=['object']).columns.tolist()
print("Kolom non-numerik setelah ekstraksi fitur waktu:", non_numeric_cols_after_time_features)

cols_to_onehot = []
cols_to_drop_unhandled_object = []

for col in non_numeric_cols_after_time_features:
    print(f"Analisis kolom '{col}': {df[col].nunique()} nilai unik.")
    # Cek jumlah nilai unik untuk memutuskan One-Hot Encode atau Hapus
    if df[col].nunique() < 100: # Batas arbitrari, sesuaikan
        cols_to_onehot.append(col)
        # Isi NaN di kolom kategorikal dengan 'Unknown' sebelum One-Hot Encoding
        if df[col].isnull().any():
            df[col] = df[col].fillna('Unknown')
            print(f"Mengisi NaN di '{col}' dengan 'Unknown' sebelum One-Hot Encoding.")
    else:
        print(f"WARNING: Kolom '{col}' memiliki banyak nilai unik ({df[col].nunique()}), akan dihapus.")
        cols_to_drop_unhandled_object.append(col) # Hapus jika terlalu banyak unik (misal ProviderID, RoomNumber jika banyak)

# Lakukan One-Hot Encoding untuk kolom yang dipilih
if cols_to_onehot:
    print(f"\nMelakukan One-Hot Encoding pada kolom: {cols_to_onehot}")
    # dummy_na=False karena NaN sudah diisi 'Unknown'
    df = pd.get_dummies(df, columns=cols_to_onehot, drop_first=True, dummy_na=False)
    print("Shape setelah One-Hot Encoding lanjutan:", df.shape)
else:
    print("\nTidak ada kolom tambahan yang di-One-Hot Encode di langkah ini.")

"""## Pemilihan Fitur (X) dan Target (y)"""

cols_to_drop_final = ['PatientID', # ID
                      'Duration', # Kolom Target (akan diambil terpisah)
                      # Kolom waktu/tanggal asli (setelah ekstraksi fitur)
                      'VisitDate', 'AppointmentTime', 'ProviderStartTime', 'ProviderEndTime',
                      'ActualArrivalTime', 'CheckInTime', 'TriageCompleteTime',
                      'TestsCompleteTime', 'DischargeTime', 'ArrivalDateTime',
                      'FirstSeenByNurseTime', 'DoctorOrProcedureStartTime', 'RegistrationTime',
                      # Kolom kategorikal asli (setelah di-one-hot encode)
                      'InsuranceType', 'ArrivalMethod', 'TriageCategory', 'ReasonForVisit', 'TestsOrdered',
                      # Kolom non-numerik yang dihapus karena banyak unik
                      ]

# Tambahkan kolom yang diputuskan untuk dihapus dari langkah penanganan non-numerik
cols_to_drop_final.extend(cols_to_drop_unhandled_object)

# Pastikan tidak ada duplikasi dan hapus kolom yang tidak ada
cols_to_drop_final = [col for col in list(set(cols_to_drop_final)) if col in df.columns]

print(f"Kolom yang akan dihapus sebelum membuat X: {cols_to_drop_final}")

# Pisahkan fitur (X) dan target (y)
y = df['Duration'] # Ambil kolom target terlebih dahulu
X = df.drop(columns=cols_to_drop_final, errors='ignore') # Hapus kolom-kolom yang tidak diinginkan dari DataFrame

"""## Final Check Tipe Data dan Imputasi di X"""

# Pastikan X hanya berisi tipe data numerik/boolean yang sesuai untuk model
# get_dummies menghasilkan uint8, yang merupakan subtype int.
X = X.select_dtypes(include=np.number) # Termasuk int, float, uint8, dll. Jika ada boolean, tambahkan 'bool'

# Cek apakah masih ada kolom non-numerik yang terlewat
non_numeric_cols_final_check = X.select_dtypes(exclude=np.number).columns # Cek yang *bukan* numerik
if len(non_numeric_cols_final_check) > 0:
    print(f"\nWARNING: Masih ada kolom non-numerik di X final: {non_numeric_cols_final_check.tolist()}")
else:
    print("Semua kolom di X final adalah numerik.")

# Cek NaN lagi di X sebelum scaling
if X.isnull().sum().sum() > 0:
    print("\nMengisi NaN yang tersisa di X dengan median sebelum scaling...")
    # Hitung median dari X sebelum di split (menggunakan seluruh data untuk robustness)
    X_medians_for_imputation_unscaled = X.median()
    X = X.fillna(X_medians_for_imputation_unscaled)
    print("Jumlah NaN di X setelah imputasi final:", X.isnull().sum().sum())

    # Simpan median dari X (unscaled) untuk imputasi di API
    # Menggunakan median dari X keseluruhan sebelum split
    import json
    print("\n--- Salin dictionary median unscaled berikut untuk median_values_for_imputation di kode Flask Anda: ---")
    # Hanya simpan median untuk kolom yang *memang* numerik di X final
    print(json.dumps(X_medians_for_imputation_unscaled[X_medians_for_imputation_unscaled.notna()].to_dict(), indent=4))

else:
     print("\nTidak ada NaN di X sebelum scaling.")
     X_medians_for_imputation_unscaled = {} # Dictionary kosong jika tidak ada NaN
     import json
     print("\n--- Tidak ada NaN di X, median_values_for_imputation di kode Flask Anda bisa menjadi dictionary kosong atau hanya untuk kolom yang mungkin missing di input API. ---")
     print("Untuk aman, pertimbangkan menyimpan median dari X_train setelah split jika Anda mengisi NaN setelah split.")
     # Atau, simpan median dari X_train setelah split jika Anda memindahkan imputasi ke setelah split
     # X_train_medians_unscaled = X_train.median().to_dict() # Ini jika imputasi NaN di X_train

# Cek NaN di target (y) - baris dengan NaN di target harus dihapus jika tidak bisa diimputasi
if y.isnull().any():
    print(f"\nWARNING: Missing values ditemukan di kolom target 'Duration'. Jumlah: {y.isnull().sum()}")
    # Hapus baris dengan NaN di target
    initial_rows = X.shape[0]
    valid_indices = y.dropna().index
    X = X.loc[valid_indices]
    y = y.loc[valid_indices]
    print(f"Menghapus {initial_rows - X.shape[0]} baris dengan NaN di target.")
    print(f"Shape X dan y setelah menghapus baris dengan NaN di target: {X.shape}, {y.shape}")

""" ## Analisis Kategori Fitur (Known vs Not Known at Start)"""

print(f"Shape X setelah final check dan imputasi: {X.shape}")
print(f"Jumlah NaN di X setelah imputasi final: {X.isnull().sum().sum()}")
print(f"Shape y setelah menghapus NaN target: {y.shape}")
print(f"Jumlah NaN di y setelah menghapus NaN target: {y.isnull().sum().sum()}")


# --- Implementasi Pemisahan Fitur Berdasarkan Nama ---
# Ambil semua kolom fitur dari DataFrame X yang sudah final
all_feature_cols = X.columns.tolist()

# Kolom-kolom yang mengindikasikan hasil dari proses atau sulit didapat di awal
# Gunakan keyword yang relevan.
# Kolom durasi yang Anda sebutkan masuk ke sini.
# RegistrationTime_missing juga cenderung bukan input awal, meskipun bukan durasi.
post_process_keywords = ['Time', 'Delay', 'Duration', '_missing'] # Tambahkan _missing

# Filter berdasarkan keyword tersebut
# Juga sertakan kolom yang secara spesifik Anda tahu sulit diinput di awal
not_known_at_start_candidates = [col for col in all_feature_cols if any(keyword.lower() in col.lower() for keyword in post_process_keywords)]

# Tambahkan kolom lain yang Anda tahu sulit diinput tapi mungkin tidak kena keyword
# Contoh: Jika ada 'ConsultationNotes_Length' yang dihitung, mungkin sulit diinput
# manually_add_to_not_known = ['ConsultationNotes_Length']
# not_known_at_start_candidates.extend(manually_add_to_not_known)
# not_known_at_start_candidates = list(set(not_known_at_start_candidates)) # Hapus duplikat jika ada

# Sisanya dianggap bisa diketahui di awal (input API)
known_at_start = [col for col in all_feature_cols if col not in not_known_at_start_candidates]

# Periksa apakah semua fitur yang Diciptakan dari tanggal/waktu (AppointmentHour, VisitDayOfWeek, dll.)
# masuk ke 'known_at_start'. Ini karena Anda BISA menghitungnya dari input tanggal/waktu awal.
# Jika mereka tidak di known_at_start, pindahkan mereka.
# Daftar ini harus sesuai dengan fitur yang Anda ekstrak
time_based_features = ['AppointmentHour', 'VisitDayOfWeek', 'VisitMonth',
                       'VisitYear', 'VisitDayOfMonth', 'IsWeekend']

for feature in time_based_features:
    if feature in not_known_at_start_candidates:
        not_known_at_start_candidates.remove(feature)
        if feature not in known_at_start: # Pastikan tidak menambah duplikat
             known_at_start.append(feature)


# Daftar final fitur yang tidak diketahui di awal
# Saring lagi agar hanya fitur yang benar-benar ada di all_feature_cols
not_known_at_start = [col for col in not_known_at_start_candidates if col in all_feature_cols]

print("\n--- Analisis Kategori Fitur ---")
print("\n✅ Fitur Diketahui di Awal (Potensi Input API):")
print(known_at_start)

print("\n⚠️ Fitur Tidak Diketahui di Awal (Kemungkinan Perlu Imputasi Median di API):")
print(not_known_at_start)

print("\n--- Daftar Fitur Final Lengkap (untuk Model):")
print(all_feature_cols) # Ini sama dengan X.columns.tolist()


# --- Simpan Daftar Fitur Final ke File JSON (Ini PENTING UNTUK API/TF.js) ---
# Gunakan daftar LENGKAP yang digunakan model (all_feature_cols)
feature_names_filename = 'feature_names.json'
with open(feature_names_filename, 'w') as f:
    json.dump(all_feature_cols, f, indent=4)
print(f"\nDaftar fitur input model disimpan sebagai '{feature_names_filename}'.")

# Anda bisa juga menyimpan daftar known_at_start dan not_known_at_start ke JSON
# untuk dokumentasi tambahan bagi tim implementasi
with open('known_at_start_features.json', 'w') as f:
    json.dump(known_at_start, f, indent=4)
with open('not_known_at_start_features.json', 'w') as f:
    json.dump(not_known_at_start, f, indent=4)
print("Daftar fitur Known/Not Known juga disimpan ke file JSON.")


# --- Lanjutkan ke Train-Test Split, Scaling, Model Training, Saving, Konversi ---
# ... (kode yang sudah ada untuk split, scaling, membuat/melatih/menyimpan model, konversi TF.js) ...

# Pastikan bagian menyimpan scaler_params.json, median_values_for_imputation.json,
# dan ohe_mapping.json menggunakan daftar LENGKAP final_feature_columns (atau all_feature_cols)
# seperti yang sudah Anda lakukan.

""" ## Train-Test Split dan Scaling"""

# 6. Train-Test Split dan Scaling
print("\nMelakukan Train-Test Split dan Scaling...")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Shape X_train: {X_train.shape}")
print(f"Shape X_test: {X_test.shape}")
print(f"Shape y_train: {y_train.shape}")
print(f"Shape y_test: {y_test.shape}")

# %%
# Asumsikan sel Train-Test Split sudah dijalankan dan X_train serta y_train sudah tersedia.

print("\nMenghitung dan menyimpan median values untuk imputasi...")

# Hitung median untuk setiap kolom di X_train (Data Training Unscaled)
# Ini adalah nilai yang paling tepat untuk digunakan sebagai default
X_train_medians_unscaled = X_train.median()

# Simpan dictionary median values JSON
median_values_filename = 'median_values_for_imputation.json'
with open(median_values_filename, 'w') as f:
    # json.dump tidak bisa langsung menangani NaN dari Series Pandas,
    # jadi kita ubah ke dictionary Python biasa dan filter NaN (jika ada)
    # Pastikan juga nilainya dalam format float standar Python
    median_values_filtered = {k: float(v) for k, v in X_train_medians_unscaled.items() if pd.notna(v)}
    json.dump(median_values_filtered, f, indent=4) # indent=4 untuk readability

print(f"Dictionary median values (dari X_train) berhasil disimpan ke '{median_values_filename}'.")

# Anda bisa juga mencetak isinya untuk verifikasi
# import json
# print("\nIsi file median_values_for_imputation.json:")
# with open(median_values_filename, 'r') as f:
#     print(json.dumps(json.load(f), indent=4))

# %%

# %%
# Asumsikan sel-sel sebelumnya sudah dijalankan dan Anda memiliki DataFrame X_train
# Asumsikan sel yang membuat dan menyimpan 'known_at_start_features.json' sudah dijalankan

# Hitung median untuk setiap kolom di X_train (Ini sudah ada)
X_train_medians_unscaled = X_train.median()

# Tampilkan hasilnya dalam bentuk Series Pandas (Ini sudah ada)
print("Median untuk setiap kolom di X_train (unscaled):")
print(X_train_medians_unscaled)

# --- Muat daftar known_at_start dari file JSON ---
import json
try:
    with open("known_at_start_features.json", "r") as f:
        known_at_start_features = json.load(f)
    print(f"\nDimuat: known_at_start_features.json ({len(known_at_start_features)} fitur)")
except FileNotFoundError:
    print("\nERROR: File 'known_at_start_features.json' tidak ditemukan. Pastikan sel pemisahan fitur sudah dijalankan.")
    known_at_start_features = [] # Set ke list kosong agar kode tidak error

# --- Filter median values hanya untuk fitur di known_at_start ---
# Gunakan .loc[] untuk memilih baris (fitur) berdasarkan daftar nama
# Pastikan hanya mengambil fitur yang ada di X_train_medians_unscaled
filtered_medians = X_train_medians_unscaled.loc[
    X_train_medians_unscaled.index.intersection(known_at_start_features)
]


# --- Cetak median dalam format dictionary/JSON hanya untuk fitur known_at_start ---
print("\n--- Dictionary median unscaled (hanya known_at_start) untuk kode Flask/API: ---")
# Filter NaN jika ada (seharusnya tidak ada jika dihitung dari X_train yang sudah diimputasi jika perlu)
print(json.dumps(filtered_medians[filtered_medians.notna()].to_dict(), indent=4))

# Opsional: Cetak juga untuk not_known_at_start jika diinginkan
try:
    with open("not_known_at_start_features.json", "r") as f:
        not_known_at_start_features = json.load(f)

    filtered_medians_not_known = X_train_medians_unscaled.loc[
        X_train_medians_unscaled.index.intersection(not_known_at_start_features)
    ]
    print("\n--- Dictionary median unscaled (hanya not_known_at_start) untuk kode Flask/API: ---")
    print(json.dumps(filtered_medians_not_known[filtered_medians_not_known.notna()].to_dict(), indent=4))

except FileNotFoundError:
    print("File 'not_known_at_start_features.json' tidak ditemukan.")


# %%

import joblib
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Data fitur berhasil diskalakan.")

# Simpan scaler (Penting untuk API)
joblib.dump(scaler, 'scaler.pkl')
print("Scaler disimpan sebagai 'scaler.pkl'.")

# Print daftar kolom final di X (setelah imputasi, sebelum scaling) untuk digunakan di API
print("\n--- Daftar fitur (kolom) final di X (unscaled) dan urutannya (untuk API): ---")
final_feature_columns = X.columns.tolist() # Gunakan kolom dari X sebelum scaling untuk nama
print(final_feature_columns)

"""# Model Training

## Membangun Model Neural Network
"""

# Buat model Neural Network
model = Sequential()
# Input shape berdasarkan jumlah fitur final
model.add(Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)))
model.add(Dropout(0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1))  # Output layer untuk regresi (prediksi Duration)

# Compile model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Early stopping untuk mencegah overfitting
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

"""## Melatih Model"""

# Fit model
print("\nMelatih model...")
history = model.fit(
    X_train_scaled, y_train, # Gunakan data SCALED untuk X
    validation_split=0.2, # Validasi dari training data
    epochs=100,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

print("\nPelatihan model selesai.")

# Asumsikan scaler sudah di-fit di Colab
# Asumsikan X (unscaled DataFrame) sudah final dan X.columns.tolist() adalah daftar fitur final

# Dapatkan rata-rata dan standar deviasi dari scaler
scaler_mean = scaler.mean_
scaler_scale = scaler.scale_ # Ini adalah std dev yang dihitung oleh StandardScaler

# Dapatkan daftar nama kolom fitur final (sesuai urutan di X_train)
feature_names = X.columns.tolist()

# Buat dictionary untuk menyimpan parameter scaler
scaler_params = {}
for i, col_name in enumerate(feature_names):
    # Pastikan nilai diubah ke tipe data standar Python (float)
    # Tambahkan penanganan untuk std dev yang mungkin nol jika semua nilai fitur sama
    scale_value = float(scaler_scale[i]) if scaler_scale[i] != 0 else 1e-8 # Gunakan nilai epsilon kecil jika std dev nol

    scaler_params[col_name] = {
        'mean': float(scaler_mean[i]),
        'scale': scale_value
    }

# Simpan dictionary parameter scaler ke file JSON
scaler_params_filename = 'scaler_params.json'
with open(scaler_params_filename, 'w') as f:
    json.dump(scaler_params, f, indent=4) # indent=4 untuk readability

print(f"Parameter scaler (mean dan scale) berhasil disimpan ke '{scaler_params_filename}' dalam format JSON.")

# Cetak juga daftar kolom untuk referensi di JavaScript
print("\n--- Daftar fitur (kolom) final (training_columns) untuk referensi di JS: ---")
print(feature_names)

# Opsional: Simpan juga dictionary median values JSON jika Anda memerlukannya untuk imputasi missing values di JavaScript
# median_values_filename = 'median_values.json'
# median_values = X_train.median().to_dict() # Gunakan median dari X_train unscaled
# with open(median_values_filename, 'w') as f:
#     json.dump(median_values, f, indent=4)
# print(f"Dictionary median values berhasil disimpan ke '{median_values_filename}' dalam format JSON.")

"""## Menyimpan Model dan Parameter Pendukung"""

# Simpan model lengkap (arsitektur + bobot)
model.save('my_model.h5')
print("Model disimpan sebagai 'my_model.h5'.")

# # Instal tensorflowjs converter
# !pip install -U tensorflowjs

# Pastikan sel yang menyimpan model.h5 sudah dijalankan
# model.save('my_model.h5')

# Tentukan nama file model Keras input
keras_model_path = 'my_model.h5'

# Tentukan nama direktori output untuk file TF.js
tfjs_output_dir = 'tfjs_model_duration' # Nama direktori output

# Jalankan perintah konversi menggunakan tensorflowjs_converter
# --input_format keras: Menentukan format input adalah Keras HDF5 (.h5)
# --output_format tfjs_layers_model: Menentukan format output adalah TF.js Layers Model
#                                      (cocok untuk model Sequential seperti milik Anda)
!tensorflowjs_converter --input_format keras \
                        --output_format tfjs_layers_model \
                        {keras_model_path} \
                        {tfjs_output_dir}

print(f"\nModel Keras '{keras_model_path}' telah dikonversi ke format TensorFlow.js di direktori '{tfjs_output_dir}'.")

# Untuk melihat file yang dihasilkan di direktori output
!ls {tfjs_output_dir}

""" # Model Evaluation"""

# Evaluasi di data test
y_pred = model.predict(X_test_scaled)

# Hitung metrik evaluasi
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"MAE: {mae:.2f} menit")
print(f"RMSE: {rmse:.2f} menit")
print(f"R^2 Score: {r2:.4f}")

""" ## Perbandingan Prediksi dan Data Asli"""

# Bandingkan beberapa contoh prediksi dengan data asli
print("\nPerbandingan 10 Contoh Prediksi vs Asli:")
perbandingan = pd.DataFrame({
    'Durasi_Asli (menit)': y_test.values[:10],
    'Durasi_Prediksi (menit)': y_pred[:10].flatten()
})
# Opsional: Terapkan post-processing (max(0, pred)) untuk tampilan perbandingan jika diinginkan
# perbandingan['Durasi_Prediksi (menit)'] = perbandingan['Durasi_Prediksi (menit)'].apply(lambda x: max(0, x))
perbandingan['Error (menit)'] = perbandingan['Durasi_Asli (menit)'] - perbandingan['Durasi_Prediksi (menit)']
display(perbandingan)

"""## Visualisasi Hasil Evaluasi"""

# Plot loss selama training
plt.figure(figsize=(10,5))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title("Loss Selama Training")
plt.xlabel("Epoch")
plt.ylabel("Loss (MSE)")
plt.legend()
plt.grid()
plt.show()


# Plot hasil prediksi vs data asli
plt.figure(figsize=(6,6))
sns.scatterplot(x=y_test, y=y_pred.flatten(), alpha=0.6) # flatten y_pred
plt.xlabel("Duration Asli (menit)")
plt.ylabel("Duration Prediksi (menit)")
plt.title("Prediksi vs Asli")
# Tambahkan garis y=x ideal
min_val = min(y_test.min(), y_pred.min())
max_val = max(y_test.max(), y_pred.max())
plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)
plt.grid(True)
plt.show()

# Generate requirements.txt
!pip freeze > requirements.txt

from google.colab import files
files.download("requirements.txt")

"""# Kesimpulan dan Langkah Selanjutnya

### Model Neural Network telah dilatih untuk memprediksi durasi penanganan oleh provider. Model dan artefak pendukung (scaler params, feature names) sudah disimpan dan dikonversi ke format TF.js, siap untuk di-deploy ke lingkungan lain seperti aplikasi web atau backend API.

# Model Anda saat ini dilatih untuk memprediksi berapa lama provider akan menangani pasien setelah mereka mulai ditangani.
"""